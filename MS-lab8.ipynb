{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d013f059-30c0-4042-aba3-a6e0052617d0",
   "metadata": {},
   "source": [
    "# Modeling and Simulation 8 - Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85399bbb-bc7c-489c-ab03-d6527bce3074",
   "metadata": {},
   "source": [
    "## The Sequential Quadratic Programming (SQP) Algorithm\n",
    "\n",
    "Quadratic programming (QP) is a type of mathematical optimization problem where the objective function and constraints are quadratic. It involves minimizing or maximizing a quadratic objective function subject to linear equality or inequality constraints.\n",
    "\n",
    "The general form of a quadratic programming problem can be expressed as:\n",
    "\n",
    "Minimize or maximize:\n",
    "0.5 * x^T * P * x + q^T * x\n",
    "\n",
    "Subject to:\n",
    "G * x <= h\n",
    "A * x = b\n",
    "\n",
    "Here, x is the vector of decision variables, P is a symmetric positive definite matrix representing the quadratic term, q is the vector representing the linear term, G and h are the matrices and vectors defining the inequality constraints, and A and b are the matrices and vectors defining the equality constraints.\n",
    "\n",
    "Quadratic programming problems can be classified into two main categories based on the properties of the matrices P and G:\n",
    "\n",
    "- Convex QP: In a convex QP, the matrix P is positive semidefinite, which means that the objective function is convex. Convex QP problems have a single global minimum or maximum solution, and efficient algorithms exist to solve them, such as the interior-point method or active-set methods.\n",
    "\n",
    "- Non-convex QP: Non-convex QP problems have a matrix P that is not positive semidefinite, resulting in a non-convex objective function. Non-convex QPs can have multiple local minima or maxima, and finding the global solution becomes more challenging. Various algorithms, including heuristic approaches, global optimization techniques, or branch-and-bound methods, are used to solve non-convex QP problems.\n",
    "\n",
    "Quadratic programming finds applications in various fields, including finance, engineering, economics, operations research, and machine learning. Some common applications of quadratic programming include:\n",
    "\n",
    "* Portfolio optimization in finance, where investors aim to find the optimal allocation of assets to maximize returns while considering risk factors.\n",
    "* Support vector machines (SVMs) in machine learning, where quadratic programming is used to find the hyperplane that separates classes with the maximum margin.\n",
    "* Process optimization in engineering, where quadratic programming is used to optimize process parameters while satisfying constraints.\n",
    "* Resource allocation and scheduling problems, where quadratic programming is used to allocate resources optimally while considering various constraints.\n",
    "\n",
    "Solving quadratic programming problems often requires specialized optimization solvers or libraries that can handle the specific problem structure efficiently. Many numerical computing environments, such as MATLAB, Python with libraries like SciPy or CVXPY, and optimization solvers like Gurobi or CPLEX, provide tools for solving quadratic programming problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b6a3d-8993-4d9c-9f6c-572bf405adba",
   "metadata": {},
   "source": [
    "Here's the corrected formatting of the block of text:\n",
    "\n",
    "Consider a nonlinear programming problem of the form:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{x} \\quad & f(x) \\\\\n",
    "\\text{subject to} \\quad & h(x) \\geq 0 \\\\\n",
    "& g(x) = 0.\n",
    "\\end{align*}\n",
    "\n",
    "The Lagrangian for this problem is:\n",
    "\n",
    "\\mathcal{L}(x, \\lambda, \\sigma) = f(x) - \\lambda h(x) - \\sigma g(x),\n",
    "\n",
    "where $\\lambda$ and $\\sigma$ are Lagrange multipliers.\n",
    "\n",
    "The standard Newton's Method searches for the solution by iterating the following equation:\n",
    "\n",
    "\\begin{bmatrix}x_{k+1}\\\\\\lambda_{k+1}\\\\\\sigma_{k+1}\\end{bmatrix} = \\begin{bmatrix}x_{k}\\\\\\lambda_{k}\\\\\\sigma_{k}\\end{bmatrix} - \\underbrace{\\begin{bmatrix}\\nabla_{xx}^{2}\\mathcal{L} & \\nabla h & \\nabla g \\\\ \\nabla h^{T} & 0 & 0 \\\\ \\nabla g^{T} & 0 & 0\\end{bmatrix}^{-1}}_{\\nabla^{2}\\mathcal{L}} \\underbrace{\\begin{bmatrix}\\nabla f + \\lambda_{k}\\nabla h + \\sigma_{k}\\nabla g \\\\ h \\\\ g\\end{bmatrix}}_{\\nabla\\mathcal{L}}.\n",
    "\n",
    "However, because the matrix $\\nabla^{2}\\mathcal{L}$ is generally singular (and therefore non-invertible), the Newton step $d_{k} = (\\nabla^{2}\\mathcal{L})^{-1}\\nabla\\mathcal{L}$ cannot be calculated directly.\n",
    "\n",
    "Instead, the basic sequential quadratic programming algorithm defines an appropriate search direction $d_{k}$ at an iterate $(x_{k}, \\lambda_{k}, \\sigma_{k})$, as a solution to the quadratic programming subproblem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{d} \\quad & f(x_{k}) + \\nabla f(x_{k})^{T}d + \\frac{1}{2}d^{T}\\nabla_{xx}^{2}\\mathcal{L}(x_{k}, \\lambda_{k}, \\sigma_{k})d \\\\\n",
    "\\text{s.t.} \\quad & h(x_{k}) + \\nabla h(x_{k})^{T}d \\geq 0 \\\\\n",
    "& g(x_{k}) + \\nabla g(x_{k})^{T}d = 0.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the term $f(x_{k})$ in the expression above may be left out for the minimization problem since it is constant under the $\\min_{d}$ operator.\n",
    "\n",
    "Together, the SQP algorithm starts by first choosing the initial iterate $(x_{0}, \\lambda_{0}, \\sigma_{0})$, then calculating $\\nabla^{2}\\mathcal{L}(x_{0}, \\lambda_{0}, \\sigma_{0})$ and $\\nabla\\mathcal{L}(x_{0}, \\lambda_{0}, \\sigma_{0})$. Then the QP subproblem is built and solved to find the Newton step direction $d_{0}$, which is used to update the parent problem iterate using:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\begin{align*}\n",
    "x_{k+1} \\\\\n",
    "\\lambda_{k+1} \\\\\n",
    "\\sigma_{k+1}\n",
    "\\end{align*}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_k \\\\\n",
    "\\lambda_k \\\\\n",
    "\\sigma_k\n",
    "\\end{bmatrix}\n",
    "+ d_k.\n",
    "\n",
    "The SQP algorithm iteratively performs these steps until convergence is achieved. Each iteration involves solving the QP subproblem to obtain the search direction $d_k$, updating the iterate variables, and recalculating the necessary derivatives for the next iteration.\n",
    "\n",
    "By iteratively updating the iterate variables based on the search direction, the SQP algorithm aims to find a solution that satisfies the KKT conditions for the given nonlinear programming problem. It combines aspects of Newton's method with quadratic programming to handle both equality and inequality constraints in the optimization problem.\n",
    "\n",
    "Note that the specific implementation of the SQP algorithm may vary, and additional techniques such as line search or merit function approaches may be incorporated for better convergence and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5ae24-d83c-4f2e-8aa1-e93a7acdd800",
   "metadata": {},
   "source": [
    "These is a rough outline of the SQP algorithm that we are going to study today:\n",
    "\n",
    "1. Initialize Variables:\n",
    "\n",
    "- Set the maximum number of iterations (MAX_ITER) and the tolerance for convergence (TOLERANCE).\n",
    "- Initialize the solution, lagrange_multipliers, and old_constraint_values with the given initial values.\n",
    "\n",
    "2. Iteration Loop:\n",
    "\n",
    "- Iterate for a maximum of MAX_ITER times.\n",
    "- For each iteration:\n",
    "    - Construct and solve the quadratic subproblem:\n",
    "            Compute the gradient of the objective function (objective_gradient) and the gradients of the constraint functions (constraint_gradients) at the current solution.\n",
    "            Compute the Hessian of the objective function (objective_hessian) and the Hessians of the constraint functions (constraint_hessians) at the current solution.\n",
    "            Build the augmented Lagrangian Hessian by summing the objective Hessian with the weighted constraint Hessians (weighted by the lagrange_multipliers).\n",
    "            Build the augmented Lagrangian gradient by summing the objective gradient with the weighted constraint gradients (weighted by the lagrange_multipliers).\n",
    "            Solve the quadratic subproblem by solving the equation: augmented_lagrangian_hessian * search_direction = -augmented_lagrangian_gradient, where search_direction is the search direction for the next iterate.\n",
    "    - Update the solution:\n",
    "            Perform a line search to determine the step length (step_length) by iteratively halving it until all constraint functions are satisfied at the new solution (new_solution = solution + step_length * search_direction).\n",
    "            Update the solution by assigning it the new_solution.\n",
    "    - Update the Lagrange multipliers:\n",
    "            Compute the constraint values (constraint_values) by evaluating the constraint functions at the updated solution.\n",
    "            Update the Lagrange multipliers by adding the constraint values to the lagrange_multipliers.\n",
    "    - Convergence Check:\n",
    "            Compute the maximum violation of the constraints (max_constraint_violation) by taking the maximum absolute difference between the current constraint values and the previous constraint values (old_constraint_values).\n",
    "            Update the old_constraint_values with the current constraint values for the next iteration.\n",
    "            If the maximum constraint violation is below the tolerance (max_constraint_violation < TOLERANCE), terminate the iterations and return the solution.\n",
    "    - If the maximum number of iterations is reached without convergence, print a message indicating the lack of convergence.\n",
    "\n",
    "Formally, this would mean:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\text{Compute the gradient of the objective function: } \\nabla f(\\mathbf{x}) \\\\\n",
    "& \\text{Compute the gradients of the constraint functions: } \\nabla h_i(\\mathbf{x}) \\text{ for all constraints } i \\\\\n",
    "& \\text{Compute the Hessian of the objective function: } \\nabla^2 f(\\mathbf{x}) \\\\\n",
    "& \\text{Compute the Hessians of the constraint functions: } \\nabla^2 h_i(\\mathbf{x}) \\text{ for all constraints } i \\\\\n",
    "& \\text{Build the augmented Lagrangian Hessian:} \\\\\n",
    "& \\quad \\mathbf{H} = \\nabla^2 f(\\mathbf{x}) + \\sum_{i} \\lambda_i \\nabla^2 h_i(\\mathbf{x}) \\\\\n",
    "& \\text{Build the augmented Lagrangian gradient:} \\\\\n",
    "& \\quad \\mathbf{g} = \\nabla f(\\mathbf{x}) + \\sum_{i} \\lambda_i \\nabla h_i(\\mathbf{x}) \\\\\n",
    "& \\text{Solve the quadratic subproblem:} \\\\\n",
    "& \\quad \\mathbf{s} = \\text{solve}(\\mathbf{H}, -\\mathbf{g})\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "& \\text{Set the step length: } \\text{step\\_length} = 1.0 \\\\\n",
    "& \\text{while } \\text{step\\_length} > 1e-20: \\\\\n",
    "& \\quad \\text{Update the solution:} \\\\\n",
    "& \\quad \\quad \\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\text{step\\_length} \\times \\mathbf{s} \\\\\n",
    "& \\quad \\text{Check if all constraint functions are satisfied at the new solution:} \\\\\n",
    "& \\quad \\quad \\text{if } h_i \\quad \\quad \\quad \\quad (\\mathbf{x}_{\\text{new}}) \\leq 0 \\quad \\forall i \\\\\n",
    "& \\quad \\quad \\quad \\quad \\text{break} \\\\\n",
    "& \\quad \\quad \\text{Reduce the step length: } \\text{step\\_length} = \\text{step\\_length} \\times 0.5 \\\\\n",
    "& \\text{Update the solution:} \\\\\n",
    "& \\quad \\mathbf{x} = \\mathbf{x}_{\\text{new}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "& \\text{Compute the constraint values at the updated solution:} \\\\\n",
    "& \\quad \\mathbf{c} = [h_1(\\mathbf{x}), h_2(\\mathbf{x}), \\ldots] \\\\\n",
    "& \\text{Update the Lagrange multipliers:} \\\\\n",
    "& \\quad \\boldsymbol{\\lambda} = \\boldsymbol{\\lambda} + \\mathbf{c}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "& \\text{Compute the maximum constraint violation:} \\\\\n",
    "& \\quad \\text{max\\_constraint\\_violation} = \\max(|\\mathbf{c} - \\mathbf{c}_{\\text{old}}|) \\\\\n",
    "& \\text{Update the old constraint values:} \\\\\n",
    "& \\quad \\mathbf{c}_{\\text{old}} = \\mathbf{c} \\\\\n",
    "& \\text{if } \\text{max\\_constraint\\_violation} < \\text{TOLERANCE}: \\\\\n",
    "& \\quad \\text{Print the number of iterations taken for convergence} \\\\\n",
    "& \\quad \\text{break}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b34c7a4f-ca16-4e23-98c0-c85a631e5b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can define your own objective function, constraints, gradients, and Hessians\n",
    "def objective_func(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def constraint_func_1(x):\n",
    "    return x[0] + x[1] - 1\n",
    "\n",
    "def constraint_func_2(x):\n",
    "    return x[0]**2 + x[1]**2 - 2\n",
    "\n",
    "def gradient_obj(x):\n",
    "    return np.array([2 * x[0], 2 * x[1]], dtype=float)\n",
    "\n",
    "def gradient_constraint_1(x):\n",
    "    return np.array([1, 1], dtype=float)\n",
    "\n",
    "def gradient_constraint_2(x):\n",
    "    return np.array([2 * x[0], 2 * x[1]], dtype=float)\n",
    "\n",
    "def hessian_obj(x):\n",
    "    return np.array([[2, 0], [0, 2]], dtype=float)\n",
    "\n",
    "def hessian_constraint_1(x):\n",
    "    return np.zeros((2, 2), dtype=float)\n",
    "\n",
    "def hessian_constraint_2(x):\n",
    "    return np.array([[2, 0], [0, 2]], dtype=float)\n",
    "\n",
    "# Set initial solution and Lagrange multipliers. Make sure the initial solution fits within the constraints\n",
    "initial_solution = np.array([0, 0], dtype=float)\n",
    "initial_lagrange_multipliers = np.array([0, 0], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0d739-2e79-4a98-8439-4acdbfeb376a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Generate grid points for plotting\n",
    "\n",
    "# TODO: Apply the objective function on your grid\n",
    "\n",
    "# TODO: Plot the contour of the objective function\n",
    "\n",
    "# TODO: Plot the constraints\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Objective Function and Constraints')\n",
    "\n",
    "# Set plot limits\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8102f410-36df-49e4-9c99-ae8478e2b157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import cholesky, solve\n",
    "\n",
    "def sqp_algorithm(initial_solution, initial_lagrange_multipliers, objective_func, constraint_funcs, gradient_obj, gradient_constraints, hessian_obj, hessian_constraints):\n",
    "    MAX_ITER = 100  # Maximum number of iterations\n",
    "    TOLERANCE = 1e-6  # Tolerance for convergence\n",
    "\n",
    "    solution = initial_solution  # Initialize the solution\n",
    "    lagrange_multipliers = initial_lagrange_multipliers  # Initialize the Lagrange multipliers\n",
    "    old_constraint_values = np.zeros(initial_solution.shape)  # Initialize the old constraint values\n",
    "\n",
    "    for iteration in range(MAX_ITER):\n",
    "        # Step 1: Construct and solve the quadratic subproblem\n",
    "        # Compute the gradients of the objective function and the constraint functions\n",
    "        objective_gradient = gradient_obj(solution) \n",
    "        constraint_gradients = [gradient_func(solution) for gradient_func in gradient_constraints]\n",
    "\n",
    "        # TODO: Compute the Hessians of the objective function and the constraint functions\n",
    "\n",
    "        # Build the augmented Lagrangian Hessian and Lagrangian gradient\n",
    "        augmented_lagrangian_hessian = objective_hessian\n",
    "        augmented_lagrangian_gradient = objective_gradient\n",
    "\n",
    "        # TODO: Add the weighted constraint Hessians and gradients to the augmented Lagrangian hessian and gradients\n",
    "\n",
    "        # TODO: Solve the quadratic subproblem using scipy.linalg to obtain the new direction to go to\n",
    "\n",
    "        # Step 2: Update the solution\n",
    "        step_length = 1.0 # The optimistic value\n",
    "        while step_length > 1e-20:  # Perform line search to determine the step length\n",
    "            # TODO: Compute a new solution by going in the new search direction with the current step length\n",
    "            # and check if you are still within the constraints. If not, use half the step length.\n",
    "\n",
    "        solution = new_solution  # Update the solution\n",
    "\n",
    "        # Step 3: Update the Lagrange multipliers\n",
    "        # TODO: Compute the constraint values and Lagrange multipliers at the new solution\n",
    "\n",
    "        # Convergence check\n",
    "        # TODO: Check if the new constraint values have changed more than TOLERANCE. If not, break\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1483642-d56b-4601-9247-ce431fad121e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = sqp_algorithm(initial_solution, initial_lagrange_multipliers, objective_func, [constraint_func_1, constraint_func_2],\n",
    "                       gradient_obj, [gradient_constraint_1, gradient_constraint_2], hessian_obj, [hessian_constraint_1, hessian_constraint_2])\n",
    "\n",
    "print(\"Optimal solution:\")\n",
    "print(result)\n",
    "print(\"Objective value:\")\n",
    "print(objective_func(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
